{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a04bd3e-125b-4fbe-a2ea-864a856ab2b3",
   "metadata": {},
   "source": [
    "# **Superstore Sales Data Cleaning**\n",
    "\n",
    "This repository contains a project for cleaning and transforming a messy **Superstore Sales Data** dataset. The dataset includes sales records from a retail business and may contain issues such as missing values, duplicates, and inconsistent formatting, which were addressed to prepare the data for analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97175ee8-c463-4523-b738-4d4e8edcd3e1",
   "metadata": {},
   "source": [
    "## 1. Package Importing\n",
    "\n",
    "To begin, we import the necessary Python packages required for data exploration and cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7400b218-9f46-4779-baa8-6483d3ac2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395bec-da0a-4681-abaf-5f8ed1524600",
   "metadata": {},
   "source": [
    "We also configure the environment to ensure the project directory is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5457bd4d-d4d5-4fbb-8fc5-82bba12c3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from superstore_sales.config import RAW_DATA_FILE, CLEANED_DATA_FILE, CLEANED_DATA_DIR\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(RAW_DATA_FILE, encoding='ISO-8859-1')\n",
    "df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660529b-6e0f-4bb5-bdec-d5720b4a826e",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration\n",
    "\n",
    "To understand the dataset, we first examine its structure and content.\n",
    "\n",
    "### 2.1 Checking Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27931c3e-4c94-465c-90b8-b44cfaec69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9994 non-null   object \n",
      " 2   Order Date     9994 non-null   object \n",
      " 3   Ship Date      9994 non-null   object \n",
      " 4   Ship Mode      9994 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9994 non-null   object \n",
      " 7   Segment        9994 non-null   object \n",
      " 8   Country        9994 non-null   object \n",
      " 9   City           9994 non-null   object \n",
      " 10  State          9994 non-null   object \n",
      " 11  Postal Code    9994 non-null   int64  \n",
      " 12  Region         9994 non-null   object \n",
      " 13  Product ID     9994 non-null   object \n",
      " 14  Category       9994 non-null   object \n",
      " 15  Sub-Category   9994 non-null   object \n",
      " 16  Product Name   9994 non-null   object \n",
      " 17  Sales          9994 non-null   float64\n",
      " 18  Quantity       9994 non-null   int64  \n",
      " 19  Discount       9994 non-null   float64\n",
      " 20  Profit         9994 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(15)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5b72b-fcd7-4727-a0ec-daf9963e6aa2",
   "metadata": {},
   "source": [
    "### 2.2 Initial Observations\n",
    "\n",
    "- There are no null values in the dataset.\n",
    "- Some columns require a data type change:\n",
    "  - Numerical values stored as `object` should be converted to `int` or `float`.\n",
    "  - Date columns should be converted to `datetime`.\n",
    "  - Categorical variables can be optimised using the `category` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2229ff49-47d2-4c78-9ecc-20a7c5b298a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9352</th>\n",
       "      <td>9353</td>\n",
       "      <td>CA-2017-148411</td>\n",
       "      <td>9/24/2017</td>\n",
       "      <td>9/26/2017</td>\n",
       "      <td>First Class</td>\n",
       "      <td>RO-19780</td>\n",
       "      <td>Rose O'Brian</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1309</td>\n",
       "      <td>CA-2016-141397</td>\n",
       "      <td>6/20/2016</td>\n",
       "      <td>6/21/2016</td>\n",
       "      <td>First Class</td>\n",
       "      <td>RC-19825</td>\n",
       "      <td>Roy Collins</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1306</td>\n",
       "      <td>CA-2016-101966</td>\n",
       "      <td>7/14/2016</td>\n",
       "      <td>7/16/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>BM-11785</td>\n",
       "      <td>Bryan Mills</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>6507</td>\n",
       "      <td>US-2016-128909</td>\n",
       "      <td>10/9/2016</td>\n",
       "      <td>10/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>SP-20545</td>\n",
       "      <td>Sibella Parks</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>3799</td>\n",
       "      <td>CA-2017-147760</td>\n",
       "      <td>11/4/2017</td>\n",
       "      <td>11/5/2017</td>\n",
       "      <td>First Class</td>\n",
       "      <td>KL-16555</td>\n",
       "      <td>Kelly Lampkin</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Row ID        Order ID Order Date   Ship Date     Ship Mode Customer ID  \\\n",
       "9352    9353  CA-2017-148411  9/24/2017   9/26/2017   First Class    RO-19780   \n",
       "1308    1309  CA-2016-141397  6/20/2016   6/21/2016   First Class    RC-19825   \n",
       "1305    1306  CA-2016-101966  7/14/2016   7/16/2016  Second Class    BM-11785   \n",
       "6506    6507  US-2016-128909  10/9/2016  10/11/2016  Second Class    SP-20545   \n",
       "3798    3799  CA-2017-147760  11/4/2017   11/5/2017   First Class    KL-16555   \n",
       "\n",
       "      Customer Name    Segment        Country  \n",
       "9352   Rose O'Brian   Consumer  United States  \n",
       "1308    Roy Collins   Consumer  United States  \n",
       "1305    Bryan Mills   Consumer  United States  \n",
       "6506  Sibella Parks  Corporate  United States  \n",
       "3798  Kelly Lampkin  Corporate  United States  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>43229</td>\n",
       "      <td>East</td>\n",
       "      <td>OFF-BI-10001267</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Universal Recycled Hanging Pressboard Report B...</td>\n",
       "      <td>12.957</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-9.5018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>65109</td>\n",
       "      <td>Central</td>\n",
       "      <td>FUR-FU-10003975</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Eldon Advantage Chair Mats for Low to Medium P...</td>\n",
       "      <td>86.620</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>New York</td>\n",
       "      <td>10009</td>\n",
       "      <td>East</td>\n",
       "      <td>FUR-FU-10001934</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Magnifier Swing Arm Lamp</td>\n",
       "      <td>83.920</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>80906</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-PA-10001166</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Paper</td>\n",
       "      <td>Xerox 1932</td>\n",
       "      <td>85.056</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28.7064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8677</th>\n",
       "      <td>Texas</td>\n",
       "      <td>76063</td>\n",
       "      <td>Central</td>\n",
       "      <td>FUR-TA-10004607</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Hon 2111 Invitation Series Straight Table</td>\n",
       "      <td>517.405</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-81.3065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         State  Postal Code   Region       Product ID         Category  \\\n",
       "3103      Ohio        43229     East  OFF-BI-10001267  Office Supplies   \n",
       "1036  Missouri        65109  Central  FUR-FU-10003975        Furniture   \n",
       "6371  New York        10009     East  FUR-FU-10001934        Furniture   \n",
       "4791  Colorado        80906     West  OFF-PA-10001166  Office Supplies   \n",
       "8677     Texas        76063  Central  FUR-TA-10004607        Furniture   \n",
       "\n",
       "     Sub-Category                                       Product Name    Sales  \\\n",
       "3103      Binders  Universal Recycled Hanging Pressboard Report B...   12.957   \n",
       "1036  Furnishings  Eldon Advantage Chair Mats for Low to Medium P...   86.620   \n",
       "6371  Furnishings                           Magnifier Swing Arm Lamp   83.920   \n",
       "4791        Paper                                         Xerox 1932   85.056   \n",
       "8677       Tables          Hon 2111 Invitation Series Straight Table  517.405   \n",
       "\n",
       "      Quantity  Discount   Profit  \n",
       "3103         7       0.7  -9.5018  \n",
       "1036         2       0.0   8.6620  \n",
       "6371         4       0.0  21.8192  \n",
       "4791         3       0.2  28.7064  \n",
       "8677         5       0.3 -81.3065  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_clean.iloc[:,0:9].sample(5))\n",
    "display(df_clean.iloc[:,10:].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3dd53ea-35d0-4fe5-a5ac-b454e8a23458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4997.500000</td>\n",
       "      <td>55190.379428</td>\n",
       "      <td>229.858001</td>\n",
       "      <td>3.789574</td>\n",
       "      <td>0.156203</td>\n",
       "      <td>28.656896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2885.163629</td>\n",
       "      <td>32063.693350</td>\n",
       "      <td>623.245101</td>\n",
       "      <td>2.225110</td>\n",
       "      <td>0.206452</td>\n",
       "      <td>234.260108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1040.000000</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6599.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.250000</td>\n",
       "      <td>23223.000000</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.728750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4997.500000</td>\n",
       "      <td>56430.500000</td>\n",
       "      <td>54.490000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7495.750000</td>\n",
       "      <td>90008.000000</td>\n",
       "      <td>209.940000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>29.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9994.000000</td>\n",
       "      <td>99301.000000</td>\n",
       "      <td>22638.480000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8399.976000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Row ID   Postal Code         Sales     Quantity     Discount  \\\n",
       "count  9994.000000   9994.000000   9994.000000  9994.000000  9994.000000   \n",
       "mean   4997.500000  55190.379428    229.858001     3.789574     0.156203   \n",
       "std    2885.163629  32063.693350    623.245101     2.225110     0.206452   \n",
       "min       1.000000   1040.000000      0.444000     1.000000     0.000000   \n",
       "25%    2499.250000  23223.000000     17.280000     2.000000     0.000000   \n",
       "50%    4997.500000  56430.500000     54.490000     3.000000     0.200000   \n",
       "75%    7495.750000  90008.000000    209.940000     5.000000     0.200000   \n",
       "max    9994.000000  99301.000000  22638.480000    14.000000     0.800000   \n",
       "\n",
       "            Profit  \n",
       "count  9994.000000  \n",
       "mean     28.656896  \n",
       "std     234.260108  \n",
       "min   -6599.978000  \n",
       "25%       1.728750  \n",
       "50%       8.666500  \n",
       "75%      29.364000  \n",
       "max    8399.976000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d95602-8e83-4971-a822-e087108acb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "      <td>9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5009</td>\n",
       "      <td>1237</td>\n",
       "      <td>1334</td>\n",
       "      <td>4</td>\n",
       "      <td>793</td>\n",
       "      <td>793</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1862</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>CA-2017-100111</td>\n",
       "      <td>9/5/2016</td>\n",
       "      <td>12/16/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>WB-21850</td>\n",
       "      <td>William Brown</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York City</td>\n",
       "      <td>California</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-PA-10001970</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Staple envelope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>5968</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>5191</td>\n",
       "      <td>9994</td>\n",
       "      <td>915</td>\n",
       "      <td>2001</td>\n",
       "      <td>3203</td>\n",
       "      <td>19</td>\n",
       "      <td>6026</td>\n",
       "      <td>1523</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Order ID Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "count             9994       9994        9994            9994        9994   \n",
       "unique            5009       1237        1334               4         793   \n",
       "top     CA-2017-100111   9/5/2016  12/16/2015  Standard Class    WB-21850   \n",
       "freq                14         38          35            5968          37   \n",
       "\n",
       "        Customer Name   Segment        Country           City       State  \\\n",
       "count            9994      9994           9994           9994        9994   \n",
       "unique            793         3              1            531          49   \n",
       "top     William Brown  Consumer  United States  New York City  California   \n",
       "freq               37      5191           9994            915        2001   \n",
       "\n",
       "       Region       Product ID         Category Sub-Category     Product Name  \n",
       "count    9994             9994             9994         9994             9994  \n",
       "unique      4             1862                3           17             1850  \n",
       "top      West  OFF-PA-10001970  Office Supplies      Binders  Staple envelope  \n",
       "freq     3203               19             6026         1523               48  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf60503-af21-49e8-8c31-27f35c99ddc4",
   "metadata": {},
   "source": [
    "#### Observations from Data Exploration\n",
    "\n",
    "1. **Data Sampling:**\n",
    "   - Two separate random samples were displayed: one from columns 0 to 9 and another from column 10 onwards.\n",
    "   - This method allows for a quick overview of different sections of the dataset.\n",
    "\n",
    "2. **Summary Statistics (`df_clean.describe()`):**\n",
    "   - The dataset contains 9,994 entries.\n",
    "   - **Sales, Profit, and Discount:**\n",
    "     - Sales have a wide range, from a minimum of 0.44 to a maximum of 22,638.48.\n",
    "     - Profit values vary significantly, from -6,599.98 to 8,399.98, indicating potential losses and gains.\n",
    "     - Discounts range from 0 to 0.8, showing varying discount strategies.\n",
    "   - **Quantity Distribution:**\n",
    "     - The quantity per transaction varies from 1 to 14, with a median of 3.\n",
    "   - **Postal Code Analysis:**\n",
    "     - The mean postal code is around 55,190, with significant variation (std = 32,063), indicating geographic diversity in the data.\n",
    "\n",
    "3. **Categorical Data Summary:**\n",
    "   - The dataset includes categorical fields such as `Order ID`, `Customer ID`, `Product ID`, `Region`, `State`, `City`, `Category`, `Sub-Category`, and `Ship Mode`.\n",
    "   - Unique counts reveal that there are 5,009 distinct orders, suggesting repeat customers or multi-product orders.\n",
    "   - The presence of unique customer IDs implies customer-level tracking.\n",
    "\n",
    "4. **Potential Areas for Further Investigation:**\n",
    "   - The large standard deviation in profit suggests significant variability in product performance.\n",
    "   - The presence of negative profits needs further exploration—certain products or regions may be underperforming.\n",
    "   - Sales and discount correlation analysis could provide insights into pricing strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adcc10e6-51d2-4d16-b1eb-71cac740e44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
       "       'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State',\n",
       "       'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category',\n",
       "       'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75937ce-c97b-4a7e-837b-365fadbc0e45",
   "metadata": {},
   "source": [
    "### 2.3 Data Validation Check  \n",
    "\n",
    "The values for the following transformations were verified to ensure correctness and consistency:  \n",
    "\n",
    "- **Row ID and Postal Code**: Converted to string format and confirmed to have no incorrect or missing values. Postal codes were checked to ensure they follow a uniform 5-digit format.  \n",
    "- **Order Date and Ship Date**: Successfully converted to datetime format, with no invalid or misformatted entries.  \n",
    "- **Categorical Columns**: Verified that 'Ship Mode', 'Segment', 'Country', 'Region', 'Category', and 'Sub-Category' contain only valid and expected values. No unexpected categories or misclassified data were found.  \n",
    "\n",
    "All transformations were validated, and the data is clean and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6d5df2-9c63-4e6f-8328-11e9eff9f8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub-Category\n",
       "Bookcases      1\n",
       "Chairs         1\n",
       "Labels         1\n",
       "Tables         1\n",
       "Storage        1\n",
       "Furnishings    1\n",
       "Art            1\n",
       "Phones         1\n",
       "Binders        1\n",
       "Appliances     1\n",
       "Paper          1\n",
       "Accessories    1\n",
       "Envelopes      1\n",
       "Fasteners      1\n",
       "Supplies       1\n",
       "Machines       1\n",
       "Copiers        1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change numbers to strings\n",
    "df_clean[['Row ID', 'Postal Code']] = df_clean[['Row ID', 'Postal Code']].astype('str')\n",
    "\n",
    "# Fill the postal codes with leading zeros to ensure a uniform 5-digit format\n",
    "df_clean['Postal Code'] = df_clean['Postal Code'].str.zfill(5)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "df_clean[['Order Date', 'Ship Date']] = df_clean[['Order Date', 'Ship Date']].apply(pd.to_datetime)\n",
    "\n",
    "# Convert selected columns to categorical data types\n",
    "df_clean['Ship Mode'] = pd.Categorical(df_clean['Ship Mode'], categories=df_clean['Ship Mode'].unique(), ordered=False)\n",
    "df_clean['Segment'] = pd.Categorical(df_clean['Segment'], categories=df_clean['Segment'].unique(), ordered=False)\n",
    "df_clean['Country'] = pd.Categorical(df_clean['Country'], categories=['United States', 'International'], ordered=False)\n",
    "df_clean['State'] = pd.Categorical(df_clean['State'], categories=df_clean['State'].unique(), ordered=False)\n",
    "df_clean['Order ID'] = pd.Categorical(df_clean['Order ID'])  \n",
    "df_clean['Customer ID'] = pd.Categorical(df_clean['Customer ID'])  \n",
    "df_clean['Postal Code'] = pd.Categorical(df_clean['Postal Code'])  \n",
    "\n",
    "\n",
    "# Ensure 'Region' is stored as a categorical variable\n",
    "df_clean['Region'] = pd.Categorical(df_clean['Region'], categories=df_clean['Region'].unique(), ordered=False)\n",
    "\n",
    "# Convert 'Category' and 'Sub-Category' to categorical types\n",
    "df_clean['Category'] = pd.Categorical(df_clean['Category'], categories=df_clean['Category'].unique(), ordered=False)\n",
    "df_clean['Sub-Category'] = pd.Categorical(df_clean['Sub-Category'], categories=df_clean['Sub-Category'].unique(), ordered=False)\n",
    "\n",
    "# Check for one-to-one relationships between IDs or other key columns\n",
    "\n",
    "# Verify if each 'Sub-Category' belongs to only one 'Category'\n",
    "subcat_cat_count = df_clean.groupby('Sub-Category', observed=True)['Category'].nunique()  # Count unique categories for each sub-category\n",
    "display(subcat_cat_count)  # If any count is greater than 1, the sub-category appears in multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd144454-4713-4d58-bcfc-8d8a49f7556a",
   "metadata": {},
   "source": [
    "#### Verification of 'Sub-Category' Consistency  \n",
    "\n",
    "A check was performed to ensure that each **'Sub-Category'** belongs to only one **'Category'**.  \n",
    "The analysis confirmed that no 'Sub-Category' appears in more than one 'Category'.  \n",
    "This validation ensures the data maintains a strict hierarchical relationship between categories and sub-categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee78db2-a1d7-41be-8533-cd5d673e456e",
   "metadata": {},
   "source": [
    "## 3. Cleaning data\n",
    "\n",
    "### 3.1 Functions for Identifying and Updating Duplicate IDs\n",
    "#### `dup_flag(df, id_col, value_col)`\n",
    "This function identifies whether an ID appears with multiple unique values in the specified column. It returns:\n",
    "- A boolean Series indicating which rows have duplicate IDs.\n",
    "- A DataFrame containing unique combinations of `id_col` and `value_col`.\n",
    "\n",
    "This is useful for detecting inconsistencies in datasets where each ID should ideally map to a single value.\n",
    "\n",
    "#### `update_id(df, id_col, value_col)`\n",
    "This function appends a numerical suffix to duplicate IDs, ensuring uniqueness while maintaining traceability. It:\n",
    "- Calls `dup_flag()` to identify duplicates.\n",
    "- Assigns a numerical suffix to duplicate occurrences.\n",
    "- Updates the ID column by appending the suffix only for duplicates.\n",
    "- Merges the updated IDs back into the original DataFrame.\n",
    "\n",
    "This function helps standardize datasets by ensuring IDs remain unique while preserving their original structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a319bbc6-05b0-482e-8227-d36d59d8d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_flag(df, id_col, value_col):\n",
    "    \"\"\"\n",
    "    Identifies duplicate IDs based on their associated values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        id_col (str): Column containing IDs.\n",
    "        value_col (str): Column to check for uniqueness within each ID.\n",
    "\n",
    "    Returns:\n",
    "        dup_flags (pd.Series): Boolean Series indicating which rows have duplicate IDs.\n",
    "        unique_combinations (pd.DataFrame): DataFrame with unique (id_col, value_col) combinations.\n",
    "    \"\"\"\n",
    "    unique_combinations = df[[id_col, value_col]].drop_duplicates().copy()\n",
    "    dup_prod = df.groupby(id_col)[value_col].nunique()\n",
    "    dup_ids = dup_prod[dup_prod > 1].index.to_list()\n",
    "    dup_flags = df[id_col].isin(dup_ids)\n",
    "    return dup_flags, unique_combinations\n",
    "\n",
    "def update_id(df, id_col, value_col):\n",
    "    \"\"\"\n",
    "    Updates duplicate IDs by appending a numerical suffix.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        id_col (str): Column containing IDs.\n",
    "        value_col (str): Column to check for uniqueness within each ID.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated IDs and duplication flags.\n",
    "    \"\"\"\n",
    "    flags = dup_flag(df, id_col, value_col)\n",
    "    df[id_col + ' dup'] = flags[0]  # Boolean flag for duplicate IDs\n",
    "    suffixes = flags[1]  # DataFrame with unique ID-value combinations\n",
    "    suffixes[id_col + ' suffix'] = suffixes.groupby(id_col).cumcount() + 1  # Assign incremental suffix\n",
    "    new_col = id_col + ' updated'\n",
    "    suffixes[new_col] = suffixes[id_col].astype(str) + \"_\" + suffixes[id_col + ' suffix'].astype(str).str.zfill(2)\n",
    "    \n",
    "    # Drop redundant columns if they exist\n",
    "    df = df.drop(columns=[col for col in [id_col + ' suffix', new_col] if col in df.columns], axis=1)\n",
    "    \n",
    "    # Merge updated suffixes with original DataFrame\n",
    "    df_new = df.merge(suffixes, on=[id_col, value_col], how='left')\n",
    "    \n",
    "    # Keep original ID where no duplicates exist\n",
    "    df_new[new_col] = np.where(df_new[id_col + \" dup\"], df_new[new_col], df_new[id_col])\n",
    "\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5045f-3899-4625-a512-46061b2a6234",
   "metadata": {},
   "source": [
    "### 3.2 Handling Duplicate IDs  \n",
    "\n",
    "The focus was placed on the **Product ID**, where duplicate IDs were detected across different products. An updated ID was generated only for those products with duplicates, ensuring that necessary updates were applied efficiently.\n",
    "\n",
    "As for the **Customer ID**, after running `update_id(df_clean, 'Customer ID', 'Customer Name')`, the results showed that the **Customer ID** requires no modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29478349-4776-48f9-af98-cf2ece57a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=update_id(df_clean,'Product ID', 'Product Name')\n",
    "df_clean['Product ID updated'] = pd.Categorical(df_clean['Product ID updated'])  \n",
    "df_clean.drop(columns=['Product ID suffix'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41513d6b-93be-4c2f-9c65-fa03a674e3b0",
   "metadata": {},
   "source": [
    "### 3.3 Time Series Analysis and Corrections  \n",
    "\n",
    "- **Date Range Verification:**  \n",
    "  - The dataset contains **Order Dates** from **2014-01-03** to **2017-12-30**.  \n",
    "  - **Ship Dates** range from **2014-01-07** to **2018-01-05**, ensuring all orders have valid shipping records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6cbdc1-4aa0-46bd-82f8-06fa1de055a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Date Range:   2014-01-03  to  2017-12-30\n",
      " Ship Date Range:   2014-01-07  to  2018-01-05\n"
     ]
    }
   ],
   "source": [
    "# Check the Date Range\n",
    "min_order, max_order = df_clean['Order Date'].min(), df_clean['Order Date'].max()\n",
    "min_ship, max_ship = df_clean['Ship Date'].min(), df_clean['Ship Date'].max()\n",
    "\n",
    "print(f\"Order Date Range:   {min_order.strftime('%Y-%m-%d')}  to  {max_order.strftime('%Y-%m-%d')}\")\n",
    "print(f\" Ship Date Range:   { min_ship.strftime('%Y-%m-%d')}  to  {max_ship.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303500b1-69a4-4b12-bdff-062591969111",
   "metadata": {},
   "source": [
    "- **Invalid Date Check:**  \n",
    "  - No instances were found where an order was shipped before it was placed. This confirms data integrity in shipping timelines.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b699bdf-0ae9-4dbe-982f-ef1c3bfc866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All shipping dates are valid. No orders were shipped before the order date.\n"
     ]
    }
   ],
   "source": [
    "# Check for Orders Shipped Before They Were Ordered\n",
    "df_invalid_dates = df_clean[df_clean['Ship Date'] < df_clean['Order Date']]\n",
    "\n",
    "if df_invalid_dates.empty:\n",
    "    print(\"All shipping dates are valid. No orders were shipped before the order date.\")\n",
    "else:\n",
    "    print(\"Orders with invalid shipping dates found:\")\n",
    "    print(df_invalid_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9aabe-2f75-4a87-812f-58de1196a476",
   "metadata": {},
   "source": [
    "- **Shipping Duration Analysis:**  \n",
    "  - The average shipping time is **~4 days**, with a minimum of **0 days** (same-day shipping) and a maximum of **7 days**.  \n",
    "  - The distribution appears reasonable, with no extreme outliers affecting the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0b14f5-4f22-40b2-937a-3762db58a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Shipping Duration Stats (in days):\n",
      "count    9994.000000\n",
      "mean        3.958175\n",
      "std         1.747567\n",
      "min         0.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max         7.000000\n",
      "Name: Shipping Duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check for Outliers (Extremely Long Shipping Times)\n",
    "df_clean['Shipping Duration'] = df_clean['Ship Date'] - df_clean['Order Date']\n",
    "print(\"\\n📊 Shipping Duration Stats (in days):\")\n",
    "print(df_clean['Shipping Duration'].dt.days.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2a326-7520-40ab-8409-e29cee774bac",
   "metadata": {},
   "source": [
    "## 4. Saving Cleaned Superstore Sales Data in Multiple Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e49a42-95ad-4cda-8108-2e5fa7924830",
   "metadata": {},
   "source": [
    "This script ensures that the cleaned **Superstore Sales** dataset is saved in multiple formats—CSV, JSON, Parquet, and Pickle—allowing flexibility for future data processing and analysis.  \n",
    "\n",
    "### 4.1 Functionality  \n",
    "1. **Creates the Cleaned Data Directory**  \n",
    "   - Uses `CLEANED_DATA_DIR` from the `superstore_sales.config` module.  \n",
    "   - Ensures the directory exists before saving files.  \n",
    "\n",
    "2. **Defines File Paths**  \n",
    "   - Specifies locations for storing the cleaned dataset and its metadata.  \n",
    "   - File types include:\n",
    "     - CSV (`SuperStoreOrders_clean.csv`)\n",
    "     - JSON (`SuperStoreOrders_clean_dtypes.json`)\n",
    "     - Parquet (`SuperStoreOrders_clean.parquet`)\n",
    "     - Pickle (`SuperStoreOrders_clean.pkl`)  \n",
    "\n",
    "3. **Saves the Cleaned Data in Different Formats**  \n",
    "   - **CSV:** A widely supported format, but does not preserve data types.  \n",
    "   - **JSON:** Stores column data types to restore them when reloading the CSV.  \n",
    "   - **Parquet:** Optimized for fast loading and maintains data types.  \n",
    "   - **Pickle:** Fully preserves the dataset but is Python-specific.  \n",
    "\n",
    "#### Output Messages  \n",
    "Each save operation prints a message confirming that the file has been successfully written.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf00c90-849a-4f5e-a0f7-6ade08c47dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in CSV format.\n",
      "Data saved in json format.\n",
      "Data saved in Parquet format.\n",
      "Data saved in Pickle format.\n"
     ]
    }
   ],
   "source": [
    "from superstore_sales.config import CLEANED_DATA_DIR\n",
    "\n",
    "\n",
    "os.makedirs(CLEANED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "csv_file = os.path.join(CLEANED_DATA_DIR, 'SuperStoreOrders_clean.csv')\n",
    "dtype_file = os.path.join(CLEANED_DATA_DIR, 'SuperStoreOrders_clean_dtypes.json')\n",
    "parquet_file = os.path.join(CLEANED_DATA_DIR, 'SuperStoreOrders_clean.parquet')\n",
    "pickle_file = os.path.join(CLEANED_DATA_DIR, 'SuperStoreOrders_clean.pkl')\n",
    "\n",
    "# Save as CSV\n",
    "df_clean.to_csv(csv_file, index=False)\n",
    "print(\"Data saved in CSV format.\")\n",
    "\n",
    "# Save column data types as JSON for CSV restoration\n",
    "df_clean.dtypes.apply(lambda x: str(x)).to_json(dtype_file)\n",
    "print(\"Data saved in json format.\")\n",
    "\n",
    "# Save as Parquet\n",
    "df_clean.to_parquet(parquet_file)\n",
    "print(\"Data saved in Parquet format.\")\n",
    "\n",
    "# Save as Pickle\n",
    "df_clean.to_pickle(pickle_file)\n",
    "print(\"Data saved in Pickle format.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e78860-d590-48a8-b14e-b35eddfc9913",
   "metadata": {},
   "source": [
    "# Comprehensive Conclusion: Superstore Sales Data Cleaning and Preparation\n",
    "\n",
    "The **Superstore Sales** dataset has been successfully cleaned and structured, ensuring accuracy, consistency, and reliability for further analysis.  \n",
    "\n",
    "## 1 Key Achievements  \n",
    "\n",
    "### 1.1 Data Integrity and Structure  \n",
    "- The dataset contains **9,994 records** with **21 attributes**, covering order details, customer information, product categories, sales figures, and regional data.  \n",
    "- **No missing values** were found, confirming data completeness.  \n",
    "- **Data types** were corrected for numerical, categorical, and date-related attributes to enhance processing efficiency.  \n",
    "\n",
    "### 1.2. Data Cleaning and Transformation  \n",
    "- **Inconsistent formatting** in numeric and date columns was rectified.  \n",
    "- **Categorical variables** were optimized to improve memory efficiency.  \n",
    "- **Duplicate Product IDs** were identified and resolved by assigning unique suffixes where necessary.  \n",
    "- **Time series validation** ensured all orders had valid shipping dates, with no cases of shipments occurring before order placement.  \n",
    "\n",
    "### 1.3. Descriptive Statistics and Insights  \n",
    "- **Sales** ranged from **\\$0.44** to **\\$22,638.48**, highlighting significant variance in transaction values.  \n",
    "- **Profit** variability was notable, spanning from **-\\$6,599.98** (losses) to \\$8,399.98 (gains).  \n",
    "- **Shipping durations** were analyzed, revealing an average of **4 days**, with no extreme outliers.  \n",
    "\n",
    "### 1.4. Data Storage and Export  \n",
    "- The cleaned dataset was **saved in multiple formats** (`CSV`, `JSON`, `Parquet`, `Pickle`) to ensure flexibility in future processing and analysis.  \n",
    "\n",
    "## 2 Next Steps  \n",
    "- Conduct **exploratory data analysis (EDA)** to uncover trends in sales, profit margins, and customer segments.  \n",
    "- Perform **correlation analysis** to examine relationships between sales, discounts, and profit.  \n",
    "- Develop **data visualizations and reports** to extract actionable insights for business decision-making.  \n",
    "\n",
    "The dataset is now well-prepared for advanced analysis and further investigation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4363fd4-e40c-4871-ad13-6798d8793b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
